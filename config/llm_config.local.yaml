llm:
  provider: openai
  model: gpt-3.5-turbo
  temperature: 0.7
  max_tokens: 1000
  max_retries: 3
  timeout: 30
  enable_cache: true
  cache_ttl_seconds: 3600
  cache_max_size: 1000
providers:
  openai:
    api_key: ''
    base_url: ''
    organization: ''
  anthropic:
    api_key: ''
    base_url: ''
  local:
    base_url: http://localhost:8000/v1
    api_key: dummy
    model: local-model
pricing:
  gpt-4:
    prompt: 0.03
    completion: 0.06
  gpt-4-turbo:
    prompt: 0.01
    completion: 0.03
  gpt-3.5-turbo:
    prompt: 0.0015
    completion: 0.002
  claude-3-opus:
    prompt: 0.015
    completion: 0.075
  claude-3-sonnet:
    prompt: 0.003
    completion: 0.015
  claude-3-haiku:
    prompt: 0.00025
    completion: 0.00125
  local:
    prompt: 0.0
    completion: 0.0
  mock:
    prompt: 0.0
    completion: 0.0
features:
  narrative:
    enabled: true
    temperature: 0.8
    max_tokens: 500
  ai_manager:
    enabled: true
    temperature: 0.3
    max_tokens: 300
    llm_adoption_rate: 0.2
  news:
    enabled: true
    temperature: 0.7
    max_tokens: 400
debug:
  verbose_logging: false
  save_prompts: false
  prompts_dir: ./logs/llm_prompts
  simulate_latency_ms: 0
