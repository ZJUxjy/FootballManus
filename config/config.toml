# Global LLM configuration
[llm]
model = "deepseek-ai/DeepSeek-V3.2"                   # The LLM model to use
base_url = "https://api.siliconflow.cn/v1"
api_key = "sk-qbyrsbvhbufpfuynrfhtwyjoukhaciugqkqtrwanpdcudrqq"
max_tokens = 32552                           # Maximum number of tokens in the response
temperature = 0.5                           # Controls randomness

# Optional configuration for specific LLM models
[llm.vision]
model = "Pro/zai-org/GLM-4.7"
base_url = "https://api.siliconflow.cn/v1"
api_key = "sk-qbyrsbvhbufpfuynrfhtwyjoukhaciugqkqtrwanpdcudrqq"
max_tokens = 32552
temperature = 0.5

# Feature-specific settings
[features]
# AI Manager settings
ai_manager_enabled = true
ai_manager_temperature = 0.3
ai_manager_max_tokens = 300
llm_adoption_rate = 0.2

# Narrative Engine settings
narrative_enabled = true
narrative_temperature = 0.8
narrative_max_tokens = 500

# News System settings
news_enabled = true
news_temperature = 0.7
news_max_tokens = 400
